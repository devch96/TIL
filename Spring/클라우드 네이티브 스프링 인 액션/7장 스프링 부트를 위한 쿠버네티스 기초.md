# 스프링 부트를 위한 쿠버네티스 기초

- 개발자가 자신의 일상적인 업무에서 인프라 문제에 너무 많은 시간을 할애하지 않아야 하지만 기본적인 내용은 알아야 함
- 쿠버네티스는 오케스트레이션 도구의 사실상 표준이 되었고 컨테이너의 배포에 대해 논의할 때 빠지지 않고 등장

-------

## 도커에서 쿠버네티스로의 이동

- 도커 컴포즈를 사용하면 네트워크와 스토리지 설정을 포함해 한 번에 여러 컨테이너의 배포를 관리할 수 있지만
한 대의 머신에 대해서만 사용할 수 있음
- 시스템의 확장성 및 복원력과 같은 클라우드 기본 특성이 필요한 상황에서 도커 CLI나 도커 컴포즈의 사용이 제한됨
- 도커로는 컨테이너를 개별 머신에 배포하지만 쿠버네티스로는 컨테이너를 여러 머신으로 이루어진 클러스터에 배포함으로써 확장성과 복원력을 가질 수 있음
- 쿠버네티스 주요 구성 요소
  - 클러스터
    - 컨테이너화된 애플리케이션을 실행하는 노드의 집합
    - 컨트롤 플레인을 가지고 있고 하나 이상의 작업자 노드로 이루어져 있음
  - 컨트롤 플레인
    - 파드의 라이프사이클을 정의, 배포 및 관리하기 위한 API 및 인터페이스를 제공하는 클러스터 구성 요소
    - 클러스터 관리, 스케줄링, 상태 모니터링과 같이 오케스트레이터의 일반적인 기능을 구현하는 모든 필수 요소들로 이루어져 있음
  - 작업자 노드
    - 컨테이너가 실행하고 네트워크에 연결할 수 있도록 CPU, 메모리, 네트워크 및 스토리지 등을 제공하는 물리적 또는 가상 머신
  - 파드
    - 애플리케이션 컨테이너를 감싸는 가장 작은 배포 단위

### 로컬 쿠버네티스 클러스터

- 미니큐브(minikube)는 도커를 기반으로 실행하기 때문에 도커 엔진을 먼저 시작해야 한다
- 미니큐브를 사용하면 프로파일을 통해 식별이 가능한 클러스터를 여러 개 만들고 제어할 수 있다
  - 프로파일이 지정되지 않으면 미니큐브는 기본 클러스터에 대해 수행된다

```shell
minikube stop // 기본 클러스터 스탑
minikube start --cpus 2 --memory 4g --driver docker --profile polar
kubectl get nodes
```

- 위 클러스터는 CPU, 메모리 리소스 제한
- 하나의 노드만으로 구성되어 있는데 컨트롤 플레인을 가지고 있고, 컨테이너화된 워크로드를 배포하기 위한 작업자 노드 역할을 수행
- 동일한 쿠버네티스 클라이언트(kubectl)를 사용해 다른 로컬 또는 원격 클러스터와 상호작용할 수 있음

```shell
kubectl config get-contexts
kubectl config current-context
kubectl config use-context polar
```

- 언제든지 minikube stop --profile polar, minikube start --profile polar 명령어로 중지하고 시작할 수 있음

### 로컬 클러스터에서 데이터 서비스 관리

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: polar-postgres
  labels:
    db: polar-postgres
spec:
  selector:
    matchLabels:
      db: polar-postgres
  template:
    metadata:
      labels:
        db: polar-postgres
    spec:
      containers:
        - name: polar-postgres
          image: postgres:14.12
          env:
            - name: POSTGRES_USER
              value: user
            - name: POSTGRES_PASSWORD
              value: password 
            - name: POSTGRES_DB
              value: polardb_catalog
          resources:
            requests:
              cpu: 100m
              memory: 60Mi
            limits:
              cpu: 200m
              memory: 120Mi

---

apiVersion: v1
kind: Service
metadata:
  name: polar-postgres
  labels:
    db: polar-postgres
spec:
  type: ClusterIP
  selector:
    db: polar-postgres
  ports:
  - protocol: TCP
    port: 5432
    targetPort: 5432
```

----------

## 스프링 부트를 위한 쿠버네티스 배포

### 컨테이너에서 파드로

- 파드는 쿠버네티스에서 배포의 가장 작은 단위
- 도커에서 쿠버네티스로 옮겨가면 컨테이너 관리도 파드 관리로 전환해야 함
- 파드는 가장 작은 쿠버네티스 객체이며 클러스터에서 실행 중인 컨테이너의 집합을 의미
- 일반적으로 파드 하나에 컨테이너 하나(애플리케이션)만 실행하는 것이 원칙이지만 로깅, 모니터링, 보안과 같은 추가 기능을 수행하는
헬퍼 컨테이너를 선택적으로 실행할 수 있음

### 배포를 통한 파드 제어

- 배포(deployment)는 상태를 갖지 않고 복제되는 애플리케이션의 수명주기를 관리하는 객체
  - 각 복제본은 파드로 나타냄
  - 복제본은 더 나은 복원력을 위해 클러스터에 여러 노드에 배포됨
- 도커는 컨테이너의 생성과 제거를 통해 애플리케이션 인스턴스를 직접 관리하지만 쿠버네티스는 파드를 직접 관리하지 않음
  - 배포 객체가 그 일을 함
  - 배포 객체를 사용해 애플리케이션을 배포하고 다운타임 없이 업그레이드를 실행하며 오류가 발생하면 이전 버전으로 롤백하거나 업그레이드를 일시
  중지하고 다시 시작할 수 있음
- 배포를 통해 복제도 관리할 수 있음
  - 레플리카셋(ReplicaSet)이라는 객체를 사용해 항상 원하는 수의 파드가 클러스터에서 실행되도록 함

### 스프링 부트 애플리케이션을 위한 배포 객체 생성

- 쿠버네티스에서 권장되는 방법은 YAML 형식으로 된 매니페스트 파일에 객체의 원하는 상태를 기술하는 것
  - 선언적 설정
- 쿠버네티스 매니페스트의 4개 주요 섹션
  - apiVersion
    - 특정 객체를 표현하기 위한 버전을 지정
    - 파드나 서비스 같은 핵심 자원은 버전 번호로만(v1 등) 구성된 버전 스키마를 따름
    - 배포 또는 레플리카셋 같은 다른 리소스는 그룹과 버전 번호로 이루어진(apps/v1) 버전 스키마를 따름
    - kubectl explain <object_name> 명령을 통해 사용할 API 버전을 포함해 객체에 대한 자세한 정보를 얻을 수 있음
  - kind
    - 파드, 레플리카셋, 배포, 서비스와 같이 생성하려는 쿠버네티스 객체의 유형을 지정함
    - kubectl api-resources 명령을 사용하면 클러스터에서 지원하는 모든 객체의 종류를 나열
  - metadata
    - 작성할 객체에 대한 세부 정보를 제공
    - 이름이나 분류를 위한 레이블 집합(키-값 쌍)이 포함됨
  - spec
    - 각 객체 유형에 따라 달라지는 섹션이며 원하는 설정을 선언하기 위해 사용

#### YAML을 사용한 배포 매니페스트 정의

```yaml
apiVersion: apps/v1 # 배포 객체에 대한 API 버전
kind: Deployment # 생성할 객체 유형
metadata:
  name: catalog-service # 배포의 이름
  labels:
    app: catalog-service # 배포에 추가할 레이블의 집합
spec:
  selector: # 확장할 파드를 선택하기 위해 사용할 레이블을 정의
    matchLabels:
      app: catalog-service
  template: # 파드 생성을 위한 템플릿
    metadata:
      labels: # 파드 객체에 추가되는 레이블. 셀렉터로 사용하는 것과 일치해야 함
        app: catalog-service
    spec:
      containers: # 파드 내 컨테이너의 목록
        - name: catalog-service # 파드 이름
          image: catalog-service # 컨테이너를 실행하기 위해 사용할 이미지(태그가 없으면 암묵적으로 latest)
          imagePullPolicy: IfNotPresent # 이미지가 로컬에 없는 경우에만 컨테이너 저장소에서 이미지를 다운
          ports:
            - containerPort: 9001 # 컨테이너에 의해 노출되는 포트
          env: # 파드로 전달되는 환경 변수
            - name: BPJ_JVM_THREAD_COUNT
              value: "50"
            - name: SPRING_DATASOURCE_URL
              value: jdbc:postgresql://polar-postgres/polardb_catalog
            - name: SPRING_PROFILES_ACTIVE
              value: testdata
```

- 도커와 마찬가지로 환경 변수를 사용해 애플리케이션이 연결할 인스턴스의 URL을 정의할 수 있음
  - URL의 호스트 이름(polar-postgres)는 서비스 객체의 이름
- 미니큐브는 기본적으로 로컬 컨테이너 이미지에 액세스할 수 없지만 수동으로 불러올 수 있음

```shell
minikube image load catalog-service --profile polar
kubectl apply -f k8s/deployment.yaml
kubectl get all -l app=catalog-service
kubectl logs deployment/catalog-service
```

- 스프링 부트 애플리케이션에 대한 배포를 만들어 로컬 쿠버네티스 클러스터에서 실행했으나 클러스터 안에 고립돼 있기 때문에 사용할 수 없음

---------

## 서비스 검색 및 부하 분산

### 서비스 검색 및 부하 분산의 이해

- 클라우드는 동일한 서비스를 실행하는 인스턴스를 여러 개 가질 수 있음
  - 각 서비스 인스턴스에는 고유한 IP 주소가 있음
- 자동 확장을 사용해 워크로드에 따라 애플리케이션을 자동으로 확장하거나 축소할 수 있기에 클라우드에서 프로세스 간 통신을 위해
IP 주소를 사용하긴 어려움
- DNS 레코드를 사용해 복제본에 할당된 IP 주소 중 하나로 연결되도록 라운드 로빈 방식을 고려할 수 있음
  - 토폴로지가 너무 자주 변경되기에 최적은 아님
  - 일부 DNS 구현은 만료 후에도 캐싱하기 때문에 유효하지 않은 매핑 결과를 사용할 가능성이 높음
- 클라우드 환경에서의 서비스 검색은 실행 중인 모든 서비스 인스턴스를 추적하고 해당 정보를 서비스 레지스트리에 저장해야 함
  - 새 인스턴스를 만들 때마다 레지스트리에 항목을 추가
  - 인스턴스가 중지하면 제거
- 애플리케이션이 지원 서비스를 호출해야 할 때마다 레지스트리에서 조회를 통해 어떤 IP 주소로 연결할 것인지 결정

### 클라이언트 측 서비스 검색 및 부하 분산

- 스프링 클라이언트 프로젝트는 스프링 애플리케이션에 클라이언트 측 서비스 검색을 추가할 수 있는 몇 가지 방안을 제공
  - 인기 있는 방법 중 하나는 스프링 클라우드 넷플릭스 유레카
  - 넷플릭스가 개발한 유레카 서비스 레지스트리를 기반

### 서버 측 서비스 검색 및 부하 분산

- 서버 측 서비스 검색 솔루션은 책임의 많은 부분을 배포 플랫폼이 담당하게 하고 개발자는 로직에 집중
- 자동으로 애플리케이션 인스턴스를 등록 혹은 등록 취소하고 로드 밸런서를 사용해 전략에 따라 들어오는 요청을 가용한 인스턴스 중 하나로
라우팅함
- 서비스 객체는 파드 집합을 대상으로 하고 액세스 정책을 정의하는 추상화
- 애플리케이션은 서비스 객체를 통해 노출된 파드로 연결해야 하는 경우 파드를 직접 호출하지 않고 서비스 이름을 사용함

### 쿠버네티스 서비스를 통한 스프링 부트 애플리케이션 노출

- 애플리케이션에 액세스 정책에 따라 서비스 유형을 몇 가지로 나눌 수 있음
- 클러스터IP
  - 기본 설정이며 가장 흔히 사용
  - 파드의 집합을 클러스터에 노출
  - 파드가 서로 통신할 수 있게 해줌
  - selector: 서비스를 통해 노출할 대상이 되는 파드를 찾을 때 사용하는 레이블
  - protocol; 서비스가 사용하는 네트워크 프로토콜
  - port: 서비스가 듣는 포트
  - targetPort: 서비스가 파드로 요청을 전달할 때 사용하는 포트

#### YAML로 서비스 매니페스트 정의

```yaml
apiVersion: v1
kind: Service
metadata:
  name: catalog-service
  labels:
    app: catalog-service
spec:
  type: ClusterIP
  selector:
    app: catalog-service
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9001 # 서비스의 대상이 되는 파드가 노출할 포트
```

#### 매니페스트를 사용한 서비스 객체 생성

```shell
kubectl apply -f k8s/service.yaml
kubectl get svc -l app=catalog-service
kubectl port-forward service/catalog-service 9001:80
```

-------

## 확장성과 일회성

### 일회성을 위한 조건: 빠른 시작

- 빠른 시작은 애플리케이션이 일회용이며 자주 만들어지고 삭제되는 클라우드 환경에 적합

### 일회성을 위한 조건: 우아한 종료

- 애플리케이션 인스턴스가 종료될 때에도 클라이언트에게 다운타임이나 오류가 일어나지 않고 정상적으로 실행되어야 함
- 우아한 종료가 의미하는 바는 애플리케이션이 새로운 요청을 받아들이지 않고, 데이터베이스 연결과 같은 열려 있는 리소스를 닫으면서 그 시점에 진행 중인
모든 요청을 완료하는 것
- 스프링 부트의 임베디드 서버는 모두 우아한 종료를 지우너하지만 다른 방식으로 작동
  - 톰캣, 제티, 네티는 종료 신호가 수신되면 새로운 요청을 전혀 받아들이지 않음
  - 언더토는 새로운 요청을 계속 받아들이지만 503 상태 코드로 응답함
- 기본 설정상 스프링 부트는 종료 신호를 받은 직후 서버를 중지
- server.shutdown 속성을 설정하면 우아한 종료 모드로 전환할 수 있음

```yaml
server:
  port: 9001
  shutdown: graceful # 우아한 종료 활성화

spring:
  application:
    name: catalog-service
  lifecycle:
    timeout-per-shutdown-phase: 15s # 15초의 종료 기간을 둠
```

- 애플리케이션에서 우아한 종료를 활성화하면 이에 따라 배포 매니페스트도 변경해야 함
- 파드를 종료해야 하는 경우 쿠버네티스는 SIGTERM 신호를 전송
- 스프링 부트는 이 신호를 가로채 우아한 종료를 시작
- 쿠버네티스는 우아한 종료를 위해 30초를 기다림
- 스프링 부트의 우아한 종료 기간이 쿠버네티스보다 짧기 때문에 애플리케이션이 종료 시점을 제어
- 쿠버네티스는 SIGTERM 신호를 파드로 보내면서 다른 구성요소들에게 종료되는 파드 쪽으로 요청 전달을 중지할 것을 통보
  - 두 작업이 병렬로 발생하기 때문에 짧은 시간 동안 요청을 계속 받을 수도 있음
- 쿠버네티스가 클러스터 전체에 요청 전달 중지 명령을 전파할 수 있는 충분한 시간을 확보하기 위해 SIGTERM 신호를 파드로 보내는 것을 지연하는 것

```yaml
    spec:
      containers:
        - name: catalog-service
          image: catalog-service
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command: ["sh", "-c", "sleep 5"]
```

### 스프링 부트 애플리케이션 확장

- 쿠버네티스에서 복제는 레플리카셋 객체에 의해 파드 레벨에서 처리됨
  - 배포 매니페스트를 통해 설정 가능

```yaml
spec:
  replicas: 2
  selector:
    matchLabels:
      app: catalog-service
```

```shell
kubectl apply -f k8s/deployment.yaml
kubectl get pods -l app=catalog-service
```

- 파드 하나를 삭제해도 자동으로 생성됨
- 레플리카셋 객체는 배포된 복제본 수를 계속 확인하면서 항상 원하는 상태와 일치하게 함

-----------

## 틸트를 사용한 로컬 쿠버네티스 개발

- 배포 및 서비스 매니페스트를 정의한 후 코드를 변경할 때마다 매번 컨테이너 이미지를 수동으로 재빌드하고 kubectl 클라이언트를 사용해
파드를 업데이트하는 것은 번거로운 일
- 틸트는 인프라와 관련한 많은 문제를 처리해주고 개발자로 하여금 애플리케이션의 비즈니스 로직에 더 집중할 수 있게 해줌

### 틸트를 사용한 내부 개발 루프

- 틸트는 로컬 환경에서 컨테이너화된 워크로드의 빌드, 배포, 관리 기능을 제공하는 오픈소스 도구

1. 클라우드 네이티브 빌드팩을 사용해 스프링 부트 애플리케이션을 컨테이너 이미지로 패키징
2. 이미지를 쿠버네티스 클러스터에 업로드
3. YAML 매니페스트에 선언된 모든 쿠버네티스 객체를 적용
4. 로컬 컴퓨터에서 애플리케이션에 액세스할 수 있도록 포트 전달을 활성화
5. 클러스터에서 실행 중인 애플리케이션의 로그에 쉽게 액세스

- 틸트는 틸트파일이라는 확장 가능한 설정 파일을 통해 설정할 수 있음
  - 스타라크라는 언어로 작성

```text
# 빌드
custom_build (
    # 컨테이너 이미지 이름
    ref = 'catalog-service',
    # 컨테이너 이미지를 빌드하기 위한 명령
    command = './gradlew bootBuildImage --imageName $EXPECTED_REF',
    # 윈도우
    command = 'gradlew.bat bootBuildImage --imageName %EXPECTED_REF%',
    # 새로운 빌드를 시작하기 위해 지켜봐야 하는 파일
    deps = ['build.gradle', 'src']
)

# 배포
k8s_yaml(['k8s/deployment.yaml', 'k8s/service.yaml'])
# 관리
k8s_resource('catalog-service', port_forwards=['9001'])
```

```shell
tilt up
tilt down
```
- tilt up 명령으로 시작된 프로세스는 명시적으로 중지하지 않는 한 계속 실행 됨
- 틸트가 제공하는 유용한 기능 중 하나는 편리한 GUI를 통해 틸트가 관리하는 서비스를 추적하고 애플리케이션 로그를 확인하며 수동으로 업데이트를 시작할 수 있음
- tilt down 으로 배포된 애플리케이션을 중지함

### 옥탄트를 사용한 쿠버네티스 워크로드 시각화

- 쿠버네티스 클러스터 및 애플리케이션을 조사할 수 있는 개발자 친화적인 쿠버네티스용 오픈소스 웹 인터페이스 옥탄트

----------

## 배포 파이프라인: 쿠버네티스 매니페스트 유효성 검사

- 쿠버네티스에 릴리스 후보를 성공적으로 배포하기 위해서는 자원 매니페스트가 필수적이기 때문에 반드시 정확해야 함

### 커밋 단계에서 쿠버네티스 매니페스트 검증

- 매니페스트를 통해 기술된 명세가 쿠버네티스 API를 준수하는지 확인해야 함

```shell
kubeval --strict -d k8s
```

### 깃허브 액션을 통한 쿠버네티스 매니페스트 유효성 검사 자동화

```yaml
      - name: Validate Kubernetes manifests
        uses: stefanprodan/kube-tools@v1
        with:
          kubectl: 1.24.3
          kubeval: 0.16.1
          command: kubeval --strict -d k8s
```