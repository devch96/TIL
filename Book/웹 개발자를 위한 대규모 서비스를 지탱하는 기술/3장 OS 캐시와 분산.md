# OS 캐시와 분산

----------

## OS의 캐시 구조

### OS의 캐시 구조를 알고 애플리케이션 작성하기

- OS에는 디스크 내의 데이터에 빠르게 액세스할 수 있도록 하는 구조가 갖춰져 있다.
- OS는 메모리를 이용해서 디스크 액세스를 줄임
- 원리를 알고 이를 전제로 애플리케이션을 작성하면 OS에 상당부분을 맡길 수 있다.
- Linux의 경우에는 페이지 캐시나 파일 캐시, 버퍼 캐시라고 하는 ㅐㅋ시 구조를 갖추고 있다.
- OS는 가상 메모리 구조를 갖추고 있음
  - 가상 메모리 구조는 논리적인 선형 어드레스를 물리적인 물리 어드레스로 변환하는 것

### 가상 메모리 구조

- 가상 메모리 구조가 존재하는 가장 큰 이유는 물리적인 하드웨어를 OS에서 추상화하기 위함
- 메모리에는 (0x0002123)과 같은 주소(어드레스)가 붙어 있지만 이러한 어드레스를 직접 프로그램에서 사용하게 되면 여러 곤란한 일이 일어남
- 따라서 프로세스에서 메모리를 필요로 하게 되면 어드레스를 가져오는 것이 아니라 OS가 메모리에서 비어있는 곳을 찾고 비어 있는 곳을 반환할 때
다른 어드레스를 반환함
  - 개별 프로세스에서 메모리의 어느 부분을 사용하는지 관여하지 않고 특정 번지수 부터 시작 또는 0x000 부터 시작하는 것으로 정해져있으면 다루기 쉬움
- OS는 메모리를 직접 프로세스로 넘기는 것이 아니라 일단 커널 내에서 메모리를 추상화하고 있다
  - 이것이 가상 메모리 구조
- 디스크의 경우에도 OS가 모아서 읽는 것처럼 메모리를 확보할 때도 이와 마찬가지 방식으로 1바이트씩 액세스 하는 것이 아닌 적당한(4KB 정도) 블록으로 확보해
넘김
  - 블록을 페이지라고 함
- OS는 프로세스에서 메모리를 요청받으면 페이지를 1개 이상, 필요한 만큼 페이지를 확보해서 프로세스에 넘기는 작업을 수행함

### Linux의 페이지 캐시 원리

- OS는 확보한 페이지를 메모리상에 계속 확보해두는 기능을 갖고 있음
- 프로세스가 디스크로부터 데이터를 읽어내는 과정
  1. OS는 디스크로부터 4KB 크기의 블록을 읽어냄
  2. 읽어낸 것은 메모리상에 위치시켜야 함(프로세스는 디스크에 직접 액세스할 수 없음, (가상) 메모리만 가능)
  3. OS는 해당 메모리 주소를 프로세스에 (가상 어드레스로서)알려줌
  4. 프로세스가 해당 메모리에 액세스함
- 데이터 읽기를 마친 프로세스가 더 이상 데이터가 필요 없어도 해당 데이터는 해제하지 않고 남겨둠. 이럴 경우 다른 프로세스가 같은 디스크에 액세스할 때에는
남겨두었던 페이지를 사용할 수 있으므로 디스크를 읽으러 갈 필요가 없게 됨
  - 페이지 캐시
- 커널이 한 번 할당한 메모리를 해제하지 않고 계속 남겨두는 것이 페이지 캐시의 기본
- 현대의 OS는 대체로 페이지 캐시와 비슷한 구조를 갖추고 있음
  - 두 번째 이후 액세스가 빨라짐
- OS를 계속 가동시켜 두면 메모리가 허락하는 한 디스크상의 데이터를 계속 캐싱함

### VFS

- 디스크를 조작하는 디바이스 드라이버와 OS 사이에는 파일시스템이 끼어있음
- 파일시스템 위에는 VFS(Virtual File System, 가상 파일 시스템)이라는 추상화레이어가 있음
- 파일시스템은 다양한 함수를 갖추고 있는데 그 인터페이스를 통일 하는 것이 VFS의 역할
- 또한 VFS가 페이지 캐시의 구조를 지니고 있음
- 어떤 파일시스템을 이용하더라도 어떤 디스크를 읽어내더라도 반드시 동일한 구조로 캐싱됨

### Linux는 페이지 단위로 디스크를 캐싱한다

- 디스크상에 4GB정도의 큰 파일이 있고 메모리는 2GB, 500MB 정도를 OS가 프로세스에 할당했다고 하면 1.5GB의 메모리 여유가 있는데 4GB의 파일을 캐싱할 수 있을까?
- OS는 블록 단위로 캐싱하기 때문에 가능하다
  - 특정 파일의 일부분만, 읽어낸 부분만 캐싱할 수 있음
  - 디스크를 캐싱하는 단위가 페이지이며 이는 가상 메모리의 최소단위다
- LRU
  - 메모리 여유분이 1.5GB, 파일을 4GB 전부 읽게 되면?
  - Least Recently Used
  - 가장 오래된 것을 파기하고 가장 새로운 것을 남겨놓는 형태
  - DB도 계속 구동시키면 캐시가 점점 최적화되어 가므로 가동시킨 직후보다 점점 뒤로 갈수록 부하, I/O가 내려가는 특성을 보임
- Linux는 파일을 i노드 번호라고 하는 번호로 식별하며, 해당 파일의 i노드 번호와 해당 파일의 어느 위치부터 시작할지를 나타내는 오프셋
이 두가지 값을 키로 캐싱함
  - 어떤 파일의 어느 위치를 이라는 쌍으로 캐시의 키를 관리할 수 있으므로 파일의 일부를 캐싱해갈 수 있음

### 메모리를 늘려서 I/O 부하 줄이기

- 메모리를 늘리면 캐시에 사용할 수 있는 용량이 늘어나고, 캐시에 사용할 수 있는 용량이 늘어나면 보다 많은 데이터를 캐싱할 수 있고,
많이 캐싱되면 디스크를 읽는 횟수가 줄어듬
- 데이터가 많아졌을 때의 기본 방침은 메모리를 늘려서 I/O부하를 줄이자
- 단순히 메모리만 늘린다고 되는 것은 아님

-----------------

## I/O 부하를 줄이는 방법

### 캐시를 전제로 한 I/O 줄이는 방법

- 데이터 규모에 비해 물리 메모리가 크면 전부 캐싱할 수 있음
  - 데이터의 크기에 주목
  - 데이터 압축 등
- 경제적인 비용과 밸런스를 고려
  - 메모리의 가격
  - 데이터 크기를 줄이는 것과 메모리 하나 추가하는 것의 비용 밸런스를 고려해야 함
  - 하지만 메모리가 큰 것은 가격이 기하급수적으로 늘어나므로 소프트웨어로 메모리 사용을 줄일 수 있도록 해야 함

### 복수 서버로 확장시키기

- 메모리를 늘려서 캐싱하면 좋겠지만 당연히 데이터 전부를 캐싱할 순 없음 
- 가장 먼저 복수 서버로 확장시키는 방안을 고려
- AP 서버는 기본적으로 CPU 부하를 낮추고 분산시키기 위해 서버를 확장하지만 DB 서버는 반드시 부하 때문은 아니고 오히려 캐시 용량을 늘리고자 할 때
혹은 효율을 높이고자 할 때 확장하는 경우가 많음
- 하지만 DB 서버는 '늘리면 좋다'라는 논리가 들어맞지 않음

### 단순히 대수만 늘려서는 확장성을 확보할 수 없다

- 단순히 데이터를 복사해서 대수를 늘리게 되면 애초에 캐시 용량이 부족해서 늘렸는데 그 부족한 부분도 그대로 동일하게 늘려감
  - 서버를 늘림으로써 시스템 전체로서는 아주 조금은 빨라질지도 모르지만 증설비용 대비 성능향상은 극히 부족

--------------

## 국소성을 살리는 분산

### 국소성을 고려한 분산이란?

- 국소성은 locality라고도 함
- 데이터를 그대로 복제하는 것과는 달리 데이터에 대한 액세스 패턴을 고려해서 분산시키는 것을 국소성을 고려한 분산이라고 함
- 액세스 패턴을 고려해서 분배한 경우 캐시영역을 확보할 수 있고 메모리에 올라간 데이터량이 늘어나게 됨

### 파티셔닝

- 파티셔닝은 한 대였던 DB 서버를 여러 대의 서버로 분할하는 방법
  - 간단한 것은 테이블 단위 분할
- 같이 액세스하는 경우가 많은 테이블끼리 모아서 서버로 분할함
- 다른 분할 방법으로는 테이블 데이터 분할
  - 테이블 하나를 여러 개의 작은 테이블로 분할
  - 예를 들어 id 값이 a~c 사이는 서버1, d~f 사이는 서버2

### 요청 패턴을 '섬'으로 분할

- 용도별로 시스템을 섬으로 나누는 방법
  - HTTP 요청의 User-Agent나 URL 등을 보고 통상의 사용자라면 섬1
  - 일부 API 요청이라면 섬3
  - 검색 봇이라면 섬2 등

### 페이지 캐시를 고려한 운용의 기본 규칙

- OS 기동 직후에 서버를 투입하지 않는다
  - 갑자기 배치하면 캐시가 없으므로 오직 디스크 액세스만 발생하게 됨
  - 시스템이 다운됨
  - OS를 시작해서 기동하면 자주 사용하는 DB의 파일을 한 번 cat해줌
  - 전부 메모리에 올라가고 그렇게 한 후에 로드밸런서에 편입시킴
- 성능평가나 부하시험도 캐시가 최적화된 후에 실시할 필요가 있음